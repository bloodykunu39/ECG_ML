{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py #type: ignore\n",
    "import pandas as pd #type: ignore\n",
    "import numpy as np #type:ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/zenodo_data/exams_extracted/'\n",
    "\n",
    "# path = '/home/pingal/Documents/ECG_data/'\n",
    "filename = path + 'exams.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(\"First few rows of the CSV file:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exam ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_1dAVb = []\n",
    "code_RBBB = []\n",
    "code_LBBB = []\n",
    "code_SB = []\n",
    "code_ST = []\n",
    "code_AF = []\n",
    "code_normal = []\n",
    "for index in range(df.shape[0]):\n",
    "    if df['1dAVb'][index] == True:\n",
    "        code_1dAVb.append(df['exam_id'][index])\n",
    "    if df['RBBB'][index] == True:\n",
    "        code_RBBB.append(df['exam_id'][index])\n",
    "    if df['LBBB'][index] == True:\n",
    "        code_LBBB.append(df['exam_id'][index])\n",
    "    if df['SB'][index] == True:\n",
    "        code_SB.append(df['exam_id'][index])\n",
    "    if df['ST'][index] == True:\n",
    "        code_ST.append(df['exam_id'][index])\n",
    "    if df['AF'][index] == True:\n",
    "        code_AF.append(df['exam_id'][index])\n",
    "    if df['normal_ecg'][index] == True:\n",
    "        code_normal.append(df['exam_id'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lenght of data with 1dAVb: {len(code_1dAVb)}')\n",
    "print(f'Lenght of data with RBBB: {len(code_RBBB)}')\n",
    "print(f'Lenght of data with LBBB: {len(code_LBBB)}')\n",
    "print(f'Lenght of data with SB: {len(code_SB)}')\n",
    "print(f'Lenght of data with ST: {len(code_ST)}')\n",
    "print(f'Lenght of data with AF: {len(code_AF)}')\n",
    "print(f'Lenght of data with normal ecg: {len(code_normal)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['1dAVB', 'RBBB', 'LBBB', 'SB', 'ST', 'AF', 'normal_ecg']\n",
    "size = [len(code_1dAVb), len(code_RBBB), len(code_RBBB), len(code_SB), len(code_ST), len(code_AF), len(code_normal)]\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'diseases' : diseases,\n",
    "    'size': size}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disease Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/pingal/Documents/ECG_data/'\n",
    "path = '/zenodo_data/exams_extracted/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1dAVb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_1dAVb:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_1dAVB', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LBBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_LBBB:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_LBBB', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RBBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_RBBB:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_RBBB', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_SB:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_SB', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_ST:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_ST', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_AF:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_AF', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### normal_ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "\n",
    "for index in range(0,17):\n",
    "    filename = 'exams_part' + str(index) + '.hdf5'\n",
    "\n",
    "    data = h5py.File(path+filename)\n",
    "    data_tracings = data['tracings']\n",
    "    data_id = data['exam_id']\n",
    "\n",
    "    for elem in range(data_tracings.shape[0]):\n",
    "        if data_id[elem] in code_normal:\n",
    "            counter.append(data_tracings[elem])\n",
    "\n",
    "\n",
    "np.save('data_normal', counter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1dAVB = np.load('data_1dAVB.npy')\n",
    "data_RBBB = np.load('data_RBBB.npy')\n",
    "data_LBBB = np.load('data_LBBB.npy')\n",
    "data_SB = np.load('data_SB.npy')\n",
    "data_ST = np.load('data_ST.npy')\n",
    "data_AF = np.load('data_AF.npy')\n",
    "data_normal = np.load('data_normal.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(arr):\n",
    "    selected_indices = np.random.choice(arr.shape[0], size=5000, replace=False)\n",
    "    selected_elements = arr[selected_indices, :, :]\n",
    "    return selected_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chdir to MAIN\n",
    "defalut_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoding import superposition\n",
    "from time import time\n",
    "from smoothening import coarsegrain\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data= random_choice(data_1dAVB)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    k=0\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup,16))\n",
    "       \n",
    "        np.save('zenodo_data/raw/1dAVB_svd_zenedo_raw' + '_'+str(k+i*500) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_1dAVB_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_rbb\n",
    "data= random_choice(data_RBBB)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    k=0\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/RBBB_svd_zenedo_raw' + '_'+str(i*500+k) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_RBBB_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_LBBB)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    k=0\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/LBBB_svd_zenedo_raw' + '_'+str(i*500+k) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_LBBB_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_SB)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "    k=0\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/SB_svd_zenedo_raw' + '_'+str(500*i+1) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_SB_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_ST)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "    k=0\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/ST_svd_zenedo_raw' + '_'+str(i*500+k) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_ST_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_AF)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "    k=0\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/AF_svd_zenedo_raw' + '_'+str(500*i+k) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_AF_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_normal)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    k=0\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup))\n",
    "        np.save('zenodo_data/raw/normal_svd_zenedo_raw' + '_'+str(i*500+k) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_normal_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "defalut_path = os.getcwd()\n",
    "\n",
    "from time import time\n",
    "from smoothening import coarsegrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(arr):\n",
    "    selected_indices = np.random.choice(arr.shape[0], size=5000, replace=False)\n",
    "    selected_elements = arr[selected_indices, :, :]\n",
    "    return selected_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1dAVB = np.load('data_1dAVB.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_1dAVB = np.load('data_1dAVB.npy')\n",
    "# data_RBBB = np.load('data_RBBB.npy')\n",
    "# data_LBBB = np.load('data_LBBB.npy')\n",
    "# data_SB = np.load('data_SB.npy')\n",
    "# data_ST = np.load('data_ST.npy')\n",
    "# data_AF = np.load('data_AF.npy')\n",
    "# data_normal = np.load('data_normal.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def image_matrix(data):\n",
    "    plt.plot(data)\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format = 'png')\n",
    "    plt.close()\n",
    "\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    image_array = np.array(image)\n",
    "    image_mat = image_array.mean(axis = -1)\n",
    "\n",
    "    return image_mat\n",
    "\n",
    "def image_stack_matrix(data):\n",
    "    image_stack = np.zeros_like(image_matrix(data[0]), dtype=float)\n",
    "    for iter, item in enumerate(data):\n",
    "        image_stack += image_matrix(item) \n",
    "    \n",
    "    return image_stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_stack_matrix(data_1dAVB[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= random_choice(data_1dAVB)\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(split_number):\n",
    "    start = time()\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    start = time()\n",
    "    result=[]\n",
    "    k=0\n",
    "    for j in cc:\n",
    "        sup=superposition(j,4096)\n",
    "        result.append(coarsegrain(sup,16))\n",
    "       \n",
    "        np.save('zenodo_data/raw/1dAVB_svd_zenedo_raw' + '_'+str(k+i*500) + '.npy',sup)\n",
    "        k+=1\n",
    "    result = np.array(result)\n",
    "    cc = []\n",
    "    for j in range(result.shape[0]):\n",
    "        cc.append(result[j].ravel())\n",
    "    # save as txt file\n",
    "    np.savetxt('zenodo_data/cg_16/disease_1dAVB_svd_zenedo_cg_16' + '_'+str(i) + '.txt',cc)\n",
    "    end=time()\n",
    "    print(f'Time taken for {i}th iteration is {end-start}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
