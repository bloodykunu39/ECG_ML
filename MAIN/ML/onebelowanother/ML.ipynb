{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "main_dir = r'D:\\LARGE_CNN\\ECG_ML\\MAIN'\n",
    "if main_dir not in sys.path:\n",
    "    sys.path.append(main_dir)\n",
    "from dataloader import MyCustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from model_nn import  Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disease_SB_physio_st_0.txt',\n",
       " 'disease_SB_physio_st_1.txt',\n",
       " 'disease_SB_physio_st_10.txt',\n",
       " 'disease_SB_physio_st_11.txt',\n",
       " 'disease_SB_physio_st_12.txt',\n",
       " 'disease_SB_physio_st_13.txt',\n",
       " 'disease_SB_physio_st_14.txt',\n",
       " 'disease_SB_physio_st_15.txt',\n",
       " 'disease_SB_physio_st_16.txt',\n",
       " 'disease_SB_physio_st_17.txt',\n",
       " 'disease_SB_physio_st_18.txt',\n",
       " 'disease_SB_physio_st_19.txt',\n",
       " 'disease_SB_physio_st_2.txt',\n",
       " 'disease_SB_physio_st_3.txt',\n",
       " 'disease_SB_physio_st_4.txt',\n",
       " 'disease_SB_physio_st_5.txt',\n",
       " 'disease_SB_physio_st_6.txt',\n",
       " 'disease_SB_physio_st_7.txt',\n",
       " 'disease_SB_physio_st_8.txt',\n",
       " 'disease_SB_physio_st_9.txt',\n",
       " 'disease_SR_physio_st_0.txt',\n",
       " 'disease_SR_physio_st_1.txt',\n",
       " 'disease_SR_physio_st_10.txt',\n",
       " 'disease_SR_physio_st_11.txt',\n",
       " 'disease_SR_physio_st_12.txt',\n",
       " 'disease_SR_physio_st_13.txt',\n",
       " 'disease_SR_physio_st_14.txt',\n",
       " 'disease_SR_physio_st_15.txt',\n",
       " 'disease_SR_physio_st_16.txt',\n",
       " 'disease_SR_physio_st_17.txt',\n",
       " 'disease_SR_physio_st_18.txt',\n",
       " 'disease_SR_physio_st_19.txt',\n",
       " 'disease_SR_physio_st_2.txt',\n",
       " 'disease_SR_physio_st_3.txt',\n",
       " 'disease_SR_physio_st_4.txt',\n",
       " 'disease_SR_physio_st_5.txt',\n",
       " 'disease_SR_physio_st_6.txt',\n",
       " 'disease_SR_physio_st_7.txt',\n",
       " 'disease_SR_physio_st_8.txt',\n",
       " 'disease_SR_physio_st_9.txt',\n",
       " 'disease_ST_physio_st_0.txt',\n",
       " 'disease_ST_physio_st_1.txt',\n",
       " 'disease_ST_physio_st_2.txt',\n",
       " 'disease_ST_physio_st_3.txt',\n",
       " 'disease_ST_physio_st_4.txt',\n",
       " 'disease_ST_physio_st_5.txt',\n",
       " 'disease_ST_physio_st_6.txt',\n",
       " 'disease_ST_physio_st_7.txt',\n",
       " 'disease_ST_physio_st_8.txt',\n",
       " 'disease_ST_physio_st_9.txt',\n",
       " 'main.ipynb',\n",
       " 'ML.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 3686400)\n",
      "data SB of length 250 in 0 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 1 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 2 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 3 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 4 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 5 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 6 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 7 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 8 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 9 index loaded\n",
      "(250, 3686400)\n",
      "data SB of length 250 in 10 index loaded\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data_SB_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     cc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisease_SB_physio_st_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     data_SB_list\u001b[38;5;241m.\u001b[39mappend(cc)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# if i==0:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guest1\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\Guest1\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1016\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _load_from_filelike(\n\u001b[0;32m   1017\u001b[0m         data, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, comment\u001b[38;5;241m=\u001b[39mcomment, quote\u001b[38;5;241m=\u001b[39mquote,\n\u001b[0;32m   1018\u001b[0m         imaginary_unit\u001b[38;5;241m=\u001b[39mimaginary_unit,\n\u001b[0;32m   1019\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols, skiplines\u001b[38;5;241m=\u001b[39mskiplines, max_rows\u001b[38;5;241m=\u001b[39mmax_rows,\n\u001b[0;32m   1020\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1021\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding, filelike\u001b[38;5;241m=\u001b[39mfilelike,\n\u001b[0;32m   1022\u001b[0m         byte_converters\u001b[38;5;241m=\u001b[39mbyte_converters)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "data_SB_list=[]\n",
    "for i in range(20):\n",
    "    cc = np.loadtxt('disease_SB_physio_st_'+str(i)+'.txt')\n",
    "    data_SB_list.append(cc)\n",
    "    # if i==0:\n",
    "    print(cc.shape)\n",
    "    print(f\"data SB of length {len(cc)} in {i} index loaded\")\n",
    "print(data_SB_list[0].shape)\n",
    "data_SB_list=np.reshape(data_SB_list,(5000,12*480,640))\n",
    "\n",
    "data_ST_list=[]\n",
    "for i in range(10):\n",
    "    cc = np.loadtxt('disease_ST_physio_st_'+str(i)+'.txt')\n",
    "    data_ST_list.append(cc)\n",
    "    # if i==0:\n",
    "    print(cc.shape)\n",
    "    print(f\"data ST of length {len(cc)} in {i} index loaded\")\n",
    "print(data_ST_list[0].shape)\n",
    "data_ST_list=np.reshape(data_ST_list,(5000,12*480,640))\n",
    "\n",
    "data_SR_list=[]\n",
    "for i in range(20):\n",
    "    cc = np.loadtxt('disease_SR_physio_st_'+str(i)+'.txt')\n",
    "    data_SR_list.append(cc)\n",
    "    # if i==0:\n",
    "    print(cc.shape)\n",
    "    print(f\"data SR of length {len(cc)} in {i} index loaded\")\n",
    "data_SR_list=np.reshape(data_SR_list,(5000,12*480,640))\n",
    "print(data_SR_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n",
      "(15000, 1, 480, 640)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create labels array\n",
    "labels = np.concatenate((\n",
    "    np.zeros(5000),\n",
    "    np.ones(5000),\n",
    "    np.ones(5000) * 2\n",
    "))\n",
    "\n",
    "# Concatenate image data\n",
    "images = np.concatenate((data_SB_list, data_ST_list, data_SR_list))\n",
    "\n",
    "# Reshape images array\n",
    "images = images.reshape(15000, 1, 12*480, 640)\n",
    "\n",
    "# Print lengths and shapes\n",
    "print(len(images), len(labels))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Delete unnecessary variables\n",
    "del data_SB_list, data_ST_list, data_SR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your dataset\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "del images,labels\n",
    "torch.manual_seed(52)\n",
    "train_dataset = MyCustomDataset(images_train, labels_train)\n",
    "test_dataset = MyCustomDataset(images_test, labels_test)\n",
    "del images_train, images_test, labels_train, labels_test\n",
    "# Define a DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 720 * 80, 512)  # Updated to match the new input size\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU, batch norm, and pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Output: 2880x320\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Output: 1440x160\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Output: 720x80\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Dynamically flatten the tensor\n",
    "        \n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.8914, Test Loss: 0.6572, Train Accuracy: 58.8917, Test Accuracy: 75.4211\n",
      "Epoch [2/25], Train Loss: 0.6465, Test Loss: 0.5428, Train Accuracy: 73.1417, Test Accuracy: 81.3608\n",
      "Epoch [3/25], Train Loss: 0.5092, Test Loss: 0.3934, Train Accuracy: 80.7833, Test Accuracy: 89.3506\n",
      "Epoch [4/25], Train Loss: 0.4010, Test Loss: 0.3198, Train Accuracy: 86.3083, Test Accuracy: 91.5669\n",
      "Epoch [5/25], Train Loss: 0.3062, Test Loss: 0.2610, Train Accuracy: 90.0333, Test Accuracy: 92.3094\n",
      "Epoch [6/25], Train Loss: 0.2422, Test Loss: 0.2072, Train Accuracy: 92.4333, Test Accuracy: 94.1268\n",
      "Epoch [7/25], Train Loss: 0.2019, Test Loss: 0.1830, Train Accuracy: 93.7917, Test Accuracy: 94.7030\n",
      "Epoch [8/25], Train Loss: 0.1711, Test Loss: 0.1689, Train Accuracy: 94.9583, Test Accuracy: 95.0687\n",
      "Epoch [9/25], Train Loss: 0.1575, Test Loss: 0.1645, Train Accuracy: 95.2583, Test Accuracy: 95.5009\n",
      "Epoch [10/25], Train Loss: 0.1412, Test Loss: 0.1617, Train Accuracy: 95.5250, Test Accuracy: 94.9690\n",
      "Epoch [11/25], Train Loss: 0.1181, Test Loss: 0.1523, Train Accuracy: 96.5167, Test Accuracy: 95.7004\n",
      "Epoch [12/25], Train Loss: 0.1088, Test Loss: 0.1801, Train Accuracy: 96.5250, Test Accuracy: 95.0355\n",
      "Epoch [13/25], Train Loss: 0.1013, Test Loss: 0.1773, Train Accuracy: 96.9417, Test Accuracy: 94.4371\n",
      "Epoch [14/25], Train Loss: 0.0941, Test Loss: 0.1563, Train Accuracy: 97.2167, Test Accuracy: 95.3679\n",
      "Epoch [15/25], Train Loss: 0.0784, Test Loss: 0.1500, Train Accuracy: 97.5500, Test Accuracy: 95.8998\n",
      "Epoch [16/25], Train Loss: 0.0744, Test Loss: 0.1501, Train Accuracy: 97.7583, Test Accuracy: 95.9996\n",
      "Epoch [17/25], Train Loss: 0.0625, Test Loss: 0.1508, Train Accuracy: 98.2500, Test Accuracy: 95.6671\n",
      "Epoch [18/25], Train Loss: 0.0605, Test Loss: 0.1552, Train Accuracy: 98.0667, Test Accuracy: 95.6339\n",
      "Epoch [19/25], Train Loss: 0.0542, Test Loss: 0.1564, Train Accuracy: 98.2833, Test Accuracy: 95.7004\n",
      "Epoch [20/25], Train Loss: 0.0480, Test Loss: 0.1621, Train Accuracy: 98.5000, Test Accuracy: 95.8001\n",
      "Epoch [21/25], Train Loss: 0.0447, Test Loss: 0.1440, Train Accuracy: 98.4750, Test Accuracy: 96.2655\n",
      "Epoch [22/25], Train Loss: 0.0397, Test Loss: 0.1605, Train Accuracy: 98.7667, Test Accuracy: 95.8001\n",
      "Epoch [23/25], Train Loss: 0.0362, Test Loss: 0.1646, Train Accuracy: 99.0083, Test Accuracy: 95.6339\n",
      "Epoch [24/25], Train Loss: 0.0336, Test Loss: 0.1687, Train Accuracy: 99.1000, Test Accuracy: 95.8001\n",
      "Epoch [25/25], Train Loss: 0.0306, Test Loss: 0.1632, Train Accuracy: 99.1833, Test Accuracy: 96.1325\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # Set seed for reproducibility\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model__ = SmallCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = torch.optim.SGD(model__.parameters(), lr=0.001)  \n",
    "\n",
    "num_epochs = 25\n",
    "test_losses_list = []\n",
    "train_losses_list = []\n",
    "accuracy_train_list = []\n",
    "accuracy_test_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model__.train()\n",
    "    running_loss, accuracy_train = 0.0, 0.0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model__(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        accuracy_train += accuracy(labels, outputs.argmax(dim=1))\n",
    "    train_losses_list.append(running_loss / len(train_dataloader))\n",
    "    \n",
    "    # Just calculating the test loss and accuracy\n",
    "    with torch.no_grad():\n",
    "        model__.eval()\n",
    "        test_loss, accuracy_test = 0.0, 0.0\n",
    "        for i, (images, labels) in enumerate(test_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model__(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            accuracy_test += accuracy(labels, outputs.argmax(dim=1))\n",
    "    test_losses_list.append(test_loss / len(test_dataloader))\n",
    "    \n",
    "    accuracy_train_list.append(accuracy_train / len(train_dataloader))\n",
    "    accuracy_test_list.append(accuracy_test / len(test_dataloader))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss / len(train_dataloader):.4f}, Test Loss: {test_loss / len(test_dataloader):.4f}, Train Accuracy: {accuracy_train / len(train_dataloader):.4f}, Test Accuracy: {accuracy_test / len(test_dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import accuracy_and_validation_plots\n",
    "\n",
    "#from plots import confusion_matrix_plot,classification_report_print,precision_recall_curve_plot\n",
    "# Example data: Replace these with your actual training and validation data\n",
    "epochs_= list(range(1, len(train_losses_list)+1))\n",
    "\n",
    "accuracy_and_validation_plots(epochs_, train_losses_list, test_losses_list, accuracy_train_list,accuracy_test_list).plot_figure(\"accuracy.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import confusion_matrix_plot,classification_report_print,precision_recall_curve_plot,accuracy_and_validation_plots\n",
    "from plots import ModelEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_and_validation_plots(train_losses_list, test_losses_list, accuracy_train_list, accuracy_test_list)\n",
    "y_true, y_pred=model_evaluate(model__, test_dataloader, device)\n",
    "confusion_matrix_plot(y_true, y_pred,[\"ST\", \"SB\", \"SR\"])\n",
    "# confusion_matrix_plot(model__, test_dataloader,[\"ST\", \"SB\", \"SR\",\"AF\",\"LBBB\",\"RBBB\",\"1dAVB\"])\n",
    "# classification_report_print(model__, test_dataloader)\n",
    "# precision_recall_curve_plot(model__, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_print(y_true, y_pred,[\"ST\", \"SB\", \"SR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "torch.save(model__.state_dict(), 'model_imagestack.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\LARGE_CNN\\\\ECG_ML\\\\MAIN\\\\zenodo_data\\\\data_stack_image'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
